{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cb1823",
   "metadata": {},
   "source": [
    "# Assignment 2: Homography Estimation\n",
    "\n",
    "**Author:** [Your Student Number]  \n",
    "**Date:** November 6, 2025\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Part 1: Feature Extraction](#part1)\n",
    "3. [Part 2: Feature Matching](#part2)\n",
    "4. [Part 3: Homography Estimation (DLT + RANSAC)](#part3)\n",
    "5. [Part 4: Image Warping and Panorama Construction](#part4)\n",
    "6. [Augmented Reality Application](#ar)\n",
    "7. [Results and Visualizations](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0052154",
   "metadata": {},
   "source": [
    "## Setup and Imports <a name=\"setup\"></a>\n",
    "\n",
    "Install required libraries and import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d2029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib backend for better display\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Print versions\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Define paths\n",
    "BASE_PATH = Path(\"/Users/otisvaliant/Desktop/Homography-Estimation\")\n",
    "PANORAMA_PATH = BASE_PATH / \"panorama_dataset\"\n",
    "AR_PATH = BASE_PATH / \"ar_dataset\"\n",
    "OUTPUT_PATH = BASE_PATH / \"panorama_results\"\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "(OUTPUT_PATH / \"visualizations\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nDataset paths:\")\n",
    "print(f\"Panorama dataset: {PANORAMA_PATH}\")\n",
    "print(f\"AR dataset: {AR_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "\n",
    "# List available scenes\n",
    "scenes = [d.name for d in PANORAMA_PATH.iterdir() if d.is_dir()]\n",
    "print(f\"\\nAvailable scenes: {scenes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e45176",
   "metadata": {},
   "source": [
    "## Part 1: Feature Extraction <a name=\"part1\"></a>\n",
    "\n",
    "Implement SIFT, SURF, and ORB feature detectors and descriptors. We'll create a unified interface for all three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc91ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Unified interface for SIFT, SURF, and ORB feature detection and description.\n",
    "    \n",
    "    Properties:\n",
    "    - SIFT: Scale-Invariant Feature Transform - robust to scale and rotation, uses 128-dim descriptors\n",
    "    - SURF: Speeded Up Robust Features - faster than SIFT, uses 64 or 128-dim descriptors\n",
    "    - ORB: Oriented FAST and Rotated BRIEF - very fast, binary descriptors (256 bits)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method: str = 'SIFT'):\n",
    "        \"\"\"\n",
    "        Initialize feature extractor with specified method.\n",
    "        \n",
    "        Args:\n",
    "            method: 'SIFT', 'SURF', or 'ORB'\n",
    "        \"\"\"\n",
    "        self.method = method.upper()\n",
    "        \n",
    "        if self.method == 'SIFT':\n",
    "            self.detector = cv2.SIFT_create()\n",
    "            self.descriptor_type = 'float'\n",
    "        elif self.method == 'SURF':\n",
    "            # SURF requires opencv-contrib-python\n",
    "            try:\n",
    "                self.detector = cv2.xfeatures2d.SURF_create()\n",
    "                self.descriptor_type = 'float'\n",
    "            except AttributeError:\n",
    "                print(\"SURF not available. Install opencv-contrib-python or use SIFT/ORB\")\n",
    "                raise\n",
    "        elif self.method == 'ORB':\n",
    "            self.detector = cv2.ORB_create(nfeatures=5000)  # Increase max features for better matching\n",
    "            self.descriptor_type = 'binary'\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}. Use 'SIFT', 'SURF', or 'ORB'\")\n",
    "    \n",
    "    def detect_and_compute(self, image: np.ndarray) -> Tuple[List, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Detect keypoints and compute descriptors.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (grayscale or color)\n",
    "            \n",
    "        Returns:\n",
    "            keypoints: List of cv2.KeyPoint objects\n",
    "            descriptors: NumPy array of descriptors\n",
    "        \"\"\"\n",
    "        # Convert to grayscale if needed\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "        \n",
    "        # Detect and compute\n",
    "        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n",
    "        \n",
    "        return keypoints, descriptors\n",
    "    \n",
    "    def visualize_keypoints(self, image: np.ndarray, keypoints: List, \n",
    "                           title: str = \"Detected Keypoints\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Visualize detected keypoints on image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            keypoints: List of cv2.KeyPoint objects\n",
    "            title: Plot title\n",
    "            \n",
    "        Returns:\n",
    "            Image with drawn keypoints\n",
    "        \"\"\"\n",
    "        img_kp = cv2.drawKeypoints(image, keypoints, None, \n",
    "                                    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
    "                                    color=(0, 255, 0))\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(cv2.cvtColor(img_kp, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"{title} - {self.method} ({len(keypoints)} keypoints)\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return img_kp\n",
    "\n",
    "print(\"FeatureExtractor class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225e42f",
   "metadata": {},
   "source": [
    "## Part 2: Feature Matching <a name=\"part2\"></a>\n",
    "\n",
    "Implement feature matching using k-NN with Lowe's ratio test for filtering ambiguous matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866633da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMatcher:\n",
    "    \"\"\"\n",
    "    Feature matcher using k-NN with Lowe's ratio test.\n",
    "    \n",
    "    Lowe's ratio test: A match is considered good if the distance to the nearest neighbor\n",
    "    is significantly smaller than the distance to the second nearest neighbor.\n",
    "    Typical threshold: 0.7-0.8\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, descriptor_type: str = 'float', ratio_threshold: float = 0.75, \n",
    "                 cross_check: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize feature matcher.\n",
    "        \n",
    "        Args:\n",
    "            descriptor_type: 'float' for SIFT/SURF, 'binary' for ORB\n",
    "            ratio_threshold: Lowe's ratio test threshold (typically 0.7-0.8)\n",
    "            cross_check: Whether to perform cross-check (match must be mutual)\n",
    "        \"\"\"\n",
    "        self.ratio_threshold = ratio_threshold\n",
    "        self.cross_check = cross_check\n",
    "        \n",
    "        # Choose appropriate matcher based on descriptor type\n",
    "        if descriptor_type == 'float':\n",
    "            # For SIFT/SURF: Use FLANN with KDTree\n",
    "            FLANN_INDEX_KDTREE = 1\n",
    "            index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "            search_params = dict(checks=50)\n",
    "            self.matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        else:\n",
    "            # For ORB: Use FLANN with LSH or BruteForce with Hamming distance\n",
    "            FLANN_INDEX_LSH = 6\n",
    "            index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, \n",
    "                               key_size=12, multi_probe_level=1)\n",
    "            search_params = dict(checks=50)\n",
    "            self.matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    def match(self, descriptors1: np.ndarray, descriptors2: np.ndarray) -> List[cv2.DMatch]:\n",
    "        \"\"\"\n",
    "        Match features between two images using k-NN and Lowe's ratio test.\n",
    "        \n",
    "        Args:\n",
    "            descriptors1: Descriptors from first image\n",
    "            descriptors2: Descriptors from second image\n",
    "            \n",
    "        Returns:\n",
    "            List of good matches (cv2.DMatch objects)\n",
    "        \"\"\"\n",
    "        if descriptors1 is None or descriptors2 is None:\n",
    "            return []\n",
    "        \n",
    "        # Ensure descriptors are in correct format\n",
    "        if descriptors1.dtype != np.float32:\n",
    "            descriptors1 = descriptors1.astype(np.float32)\n",
    "        if descriptors2.dtype != np.float32:\n",
    "            descriptors2 = descriptors2.astype(np.float32)\n",
    "        \n",
    "        # Find k=2 nearest neighbors\n",
    "        matches = self.matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
    "        \n",
    "        # Apply Lowe's ratio test\n",
    "        good_matches = []\n",
    "        for match_pair in matches:\n",
    "            if len(match_pair) == 2:\n",
    "                m, n = match_pair\n",
    "                if m.distance < self.ratio_threshold * n.distance:\n",
    "                    good_matches.append(m)\n",
    "        \n",
    "        # Optional: Cross-check\n",
    "        if self.cross_check and len(good_matches) > 0:\n",
    "            good_matches = self._cross_check_matches(descriptors1, descriptors2, good_matches)\n",
    "        \n",
    "        return good_matches\n",
    "    \n",
    "    def _cross_check_matches(self, desc1: np.ndarray, desc2: np.ndarray, \n",
    "                            matches: List[cv2.DMatch]) -> List[cv2.DMatch]:\n",
    "        \"\"\"\n",
    "        Perform cross-check: keep only matches that are mutual best matches.\n",
    "        \"\"\"\n",
    "        # Match in reverse direction\n",
    "        matches_reverse = self.matcher.knnMatch(desc2, desc1, k=2)\n",
    "        \n",
    "        # Build set of reverse matches\n",
    "        reverse_match_set = set()\n",
    "        for match_pair in matches_reverse:\n",
    "            if len(match_pair) == 2:\n",
    "                m, n = match_pair\n",
    "                if m.distance < self.ratio_threshold * n.distance:\n",
    "                    reverse_match_set.add((m.queryIdx, m.trainIdx))\n",
    "        \n",
    "        # Keep only mutual matches\n",
    "        cross_checked = []\n",
    "        for m in matches:\n",
    "            if (m.trainIdx, m.queryIdx) in reverse_match_set:\n",
    "                cross_checked.append(m)\n",
    "        \n",
    "        return cross_checked\n",
    "    \n",
    "    def visualize_matches(self, img1: np.ndarray, kp1: List, \n",
    "                         img2: np.ndarray, kp2: List, \n",
    "                         matches: List[cv2.DMatch], \n",
    "                         title: str = \"Feature Matches\",\n",
    "                         max_display: int = 100) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Visualize feature matches between two images.\n",
    "        \n",
    "        Args:\n",
    "            img1, img2: Input images\n",
    "            kp1, kp2: Keypoints\n",
    "            matches: List of matches\n",
    "            title: Plot title\n",
    "            max_display: Maximum number of matches to display\n",
    "            \n",
    "        Returns:\n",
    "            Image with drawn matches\n",
    "        \"\"\"\n",
    "        # Limit matches for visualization\n",
    "        matches_to_draw = matches[:max_display] if len(matches) > max_display else matches\n",
    "        \n",
    "        img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches_to_draw, None,\n",
    "                                      flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        \n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"{title} ({len(matches)} total matches, showing {len(matches_to_draw)})\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return img_matches\n",
    "\n",
    "print(\"FeatureMatcher class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076fd608",
   "metadata": {},
   "source": [
    "## Part 3: Homography Estimation - DLT + RANSAC <a name=\"part3\"></a>\n",
    "\n",
    "Manual implementation of Direct Linear Transform (DLT) and RANSAC algorithm for robust homography estimation.\n",
    "\n",
    "### Theory:\n",
    "- **DLT**: Solves for homography matrix H (3×3) using point correspondences\n",
    "- **Normalization**: Improves numerical stability by normalizing coordinates\n",
    "- **RANSAC**: Random Sample Consensus - iteratively fits models and finds inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e157ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomographyEstimator:\n",
    "    \"\"\"\n",
    "    Manual implementation of homography estimation using DLT and RANSAC.\n",
    "    \n",
    "    The homography H maps points from image 1 to image 2:\n",
    "    x2 = H @ x1 (in homogeneous coordinates)\n",
    "    \n",
    "    DLT minimizes ||Ah||^2 subject to ||h|| = 1, where h is the vectorized H.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ransac_iterations: int = 2000, \n",
    "                 ransac_threshold: float = 5.0,\n",
    "                 normalize: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize homography estimator.\n",
    "        \n",
    "        Args:\n",
    "            ransac_iterations: Number of RANSAC iterations\n",
    "            ransac_threshold: Inlier threshold in pixels\n",
    "            normalize: Whether to normalize point coordinates\n",
    "        \"\"\"\n",
    "        self.ransac_iterations = ransac_iterations\n",
    "        self.ransac_threshold = ransac_threshold\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def normalize_points(self, points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Normalize points for numerical stability.\n",
    "        \n",
    "        Transforms points so that:\n",
    "        - Centroid is at origin\n",
    "        - Average distance from origin is sqrt(2)\n",
    "        \n",
    "        Args:\n",
    "            points: Nx2 array of points\n",
    "            \n",
    "        Returns:\n",
    "            normalized_points: Nx2 normalized points\n",
    "            T: 3x3 normalization transformation matrix\n",
    "        \"\"\"\n",
    "        # Compute centroid\n",
    "        centroid = np.mean(points, axis=0)\n",
    "        \n",
    "        # Center points\n",
    "        centered = points - centroid\n",
    "        \n",
    "        # Compute average distance from origin\n",
    "        avg_dist = np.mean(np.sqrt(np.sum(centered**2, axis=1)))\n",
    "        \n",
    "        # Scale factor to make average distance sqrt(2)\n",
    "        if avg_dist > 0:\n",
    "            scale = np.sqrt(2) / avg_dist\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        \n",
    "        # Normalization matrix\n",
    "        T = np.array([\n",
    "            [scale, 0, -scale * centroid[0]],\n",
    "            [0, scale, -scale * centroid[1]],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        # Apply normalization\n",
    "        normalized = centered * scale\n",
    "        \n",
    "        return normalized, T\n",
    "    \n",
    "    def compute_homography_dlt(self, src_pts: np.ndarray, dst_pts: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute homography using Direct Linear Transform (DLT).\n",
    "        \n",
    "        For each correspondence (x, y) -> (x', y'), we have:\n",
    "        x' = (h11*x + h12*y + h13) / (h31*x + h32*y + h33)\n",
    "        y' = (h21*x + h22*y + h23) / (h31*x + h32*y + h33)\n",
    "        \n",
    "        This gives us 2 equations per correspondence.\n",
    "        We need at least 4 correspondences (8 equations) to solve for 8 unknowns.\n",
    "        \n",
    "        Args:\n",
    "            src_pts: Nx2 array of source points\n",
    "            dst_pts: Nx2 array of destination points\n",
    "            \n",
    "        Returns:\n",
    "            H: 3x3 homography matrix\n",
    "        \"\"\"\n",
    "        n_points = src_pts.shape[0]\n",
    "        \n",
    "        if n_points < 4:\n",
    "            raise ValueError(\"At least 4 point correspondences are required\")\n",
    "        \n",
    "        # Normalize points if requested\n",
    "        if self.normalize:\n",
    "            src_norm, T_src = self.normalize_points(src_pts)\n",
    "            dst_norm, T_dst = self.normalize_points(dst_pts)\n",
    "        else:\n",
    "            src_norm = src_pts\n",
    "            dst_norm = dst_pts\n",
    "        \n",
    "        # Build matrix A (2n x 9)\n",
    "        A = []\n",
    "        for i in range(n_points):\n",
    "            x, y = src_norm[i]\n",
    "            xp, yp = dst_norm[i]\n",
    "            \n",
    "            # Two rows per correspondence\n",
    "            A.append([-x, -y, -1, 0, 0, 0, x*xp, y*xp, xp])\n",
    "            A.append([0, 0, 0, -x, -y, -1, x*yp, y*yp, yp])\n",
    "        \n",
    "        A = np.array(A)\n",
    "        \n",
    "        # Solve using SVD: A = U S V^T\n",
    "        # Solution is the last column of V (corresponds to smallest singular value)\n",
    "        U, S, Vt = np.linalg.svd(A)\n",
    "        h = Vt[-1, :]\n",
    "        \n",
    "        # Reshape to 3x3 matrix\n",
    "        H = h.reshape(3, 3)\n",
    "        \n",
    "        # Denormalize if normalization was applied\n",
    "        if self.normalize:\n",
    "            # H = T_dst^-1 @ H_norm @ T_src\n",
    "            H = np.linalg.inv(T_dst) @ H @ T_src\n",
    "        \n",
    "        # Normalize so that H[2,2] = 1\n",
    "        H = H / H[2, 2]\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    def compute_reprojection_error(self, src_pts: np.ndarray, dst_pts: np.ndarray, \n",
    "                                   H: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute reprojection error for each point.\n",
    "        \n",
    "        Args:\n",
    "            src_pts: Nx2 source points\n",
    "            dst_pts: Nx2 destination points\n",
    "            H: 3x3 homography matrix\n",
    "            \n",
    "        Returns:\n",
    "            errors: N array of reprojection errors (Euclidean distance)\n",
    "        \"\"\"\n",
    "        n_points = src_pts.shape[0]\n",
    "        \n",
    "        # Convert to homogeneous coordinates\n",
    "        src_h = np.hstack([src_pts, np.ones((n_points, 1))])\n",
    "        \n",
    "        # Project points using H\n",
    "        projected_h = (H @ src_h.T).T\n",
    "        \n",
    "        # Convert back to Euclidean coordinates\n",
    "        projected = projected_h[:, :2] / projected_h[:, 2:3]\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        errors = np.sqrt(np.sum((projected - dst_pts)**2, axis=1))\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def estimate_homography_ransac(self, src_pts: np.ndarray, dst_pts: np.ndarray) -> Tuple:\n",
    "        \"\"\"\n",
    "        Estimate homography using RANSAC.\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Randomly select 4 correspondences\n",
    "        2. Compute homography using DLT\n",
    "        3. Count inliers (points with reprojection error < threshold)\n",
    "        4. Repeat for N iterations\n",
    "        5. Return homography with most inliers\n",
    "        \n",
    "        Args:\n",
    "            src_pts: Nx2 source points\n",
    "            dst_pts: Nx2 destination points\n",
    "            \n",
    "        Returns:\n",
    "            best_H: 3x3 homography matrix\n",
    "            inlier_mask: Boolean mask indicating inliers\n",
    "            inlier_ratio: Ratio of inliers to total points\n",
    "        \"\"\"\n",
    "        n_points = src_pts.shape[0]\n",
    "        \n",
    "        if n_points < 4:\n",
    "            raise ValueError(\"At least 4 point correspondences are required\")\n",
    "        \n",
    "        best_H = None\n",
    "        best_inliers = None\n",
    "        best_inlier_count = 0\n",
    "        \n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        \n",
    "        for iteration in range(self.ransac_iterations):\n",
    "            # Randomly select 4 points\n",
    "            indices = np.random.choice(n_points, 4, replace=False)\n",
    "            sample_src = src_pts[indices]\n",
    "            sample_dst = dst_pts[indices]\n",
    "            \n",
    "            try:\n",
    "                # Compute homography from sample\n",
    "                H = self.compute_homography_dlt(sample_src, sample_dst)\n",
    "                \n",
    "                # Compute reprojection errors for all points\n",
    "                errors = self.compute_reprojection_error(src_pts, dst_pts, H)\n",
    "                \n",
    "                # Identify inliers\n",
    "                inliers = errors < self.ransac_threshold\n",
    "                inlier_count = np.sum(inliers)\n",
    "                \n",
    "                # Update best model if this is better\n",
    "                if inlier_count > best_inlier_count:\n",
    "                    best_inlier_count = inlier_count\n",
    "                    best_inliers = inliers\n",
    "                    best_H = H\n",
    "                    \n",
    "            except np.linalg.LinAlgError:\n",
    "                # Skip degenerate configurations\n",
    "                continue\n",
    "        \n",
    "        if best_H is None:\n",
    "            raise RuntimeError(\"RANSAC failed to find a valid homography\")\n",
    "        \n",
    "        # Refine homography using all inliers\n",
    "        inlier_src = src_pts[best_inliers]\n",
    "        inlier_dst = dst_pts[best_inliers]\n",
    "        best_H = self.compute_homography_dlt(inlier_src, inlier_dst)\n",
    "        \n",
    "        # Recompute inliers with refined H\n",
    "        errors = self.compute_reprojection_error(src_pts, dst_pts, best_H)\n",
    "        best_inliers = errors < self.ransac_threshold\n",
    "        \n",
    "        inlier_ratio = np.sum(best_inliers) / n_points\n",
    "        \n",
    "        return best_H, best_inliers, inlier_ratio\n",
    "    \n",
    "    def visualize_inliers(self, img1: np.ndarray, kp1: List, \n",
    "                         img2: np.ndarray, kp2: List,\n",
    "                         matches: List[cv2.DMatch], \n",
    "                         inlier_mask: np.ndarray,\n",
    "                         title: str = \"Inliers vs Outliers\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Visualize inlier and outlier matches.\n",
    "        \n",
    "        Args:\n",
    "            img1, img2: Input images\n",
    "            kp1, kp2: Keypoints\n",
    "            matches: List of matches\n",
    "            inlier_mask: Boolean array indicating inliers\n",
    "            title: Plot title\n",
    "            \n",
    "        Returns:\n",
    "            Visualization image\n",
    "        \"\"\"\n",
    "        # Separate inliers and outliers\n",
    "        inlier_matches = [m for i, m in enumerate(matches) if inlier_mask[i]]\n",
    "        outlier_matches = [m for i, m in enumerate(matches) if not inlier_mask[i]]\n",
    "        \n",
    "        # Draw outliers in red\n",
    "        img_outliers = cv2.drawMatches(img1, kp1, img2, kp2, outlier_matches, None,\n",
    "                                       matchColor=(255, 0, 0),\n",
    "                                       flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        \n",
    "        # Draw inliers in green on top\n",
    "        img_result = cv2.drawMatches(img1, kp1, img2, kp2, inlier_matches, img_outliers,\n",
    "                                     matchColor=(0, 255, 0),\n",
    "                                     flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        \n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB))\n",
    "        n_inliers = len(inlier_matches)\n",
    "        n_total = len(matches)\n",
    "        plt.title(f\"{title} - Inliers: {n_inliers}/{n_total} ({100*n_inliers/n_total:.1f}%)\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return img_result\n",
    "\n",
    "print(\"HomographyEstimator class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca2b7b",
   "metadata": {},
   "source": [
    "## Part 4: Image Warping and Panorama Construction <a name=\"part4\"></a>\n",
    "\n",
    "Implement image warping using homography and create panoramas with blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb47378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanoramaStitcher:\n",
    "    \"\"\"\n",
    "    Image warping and panorama stitching using homography.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, blend_method: str = 'linear'):\n",
    "        \"\"\"\n",
    "        Initialize panorama stitcher.\n",
    "        \n",
    "        Args:\n",
    "            blend_method: 'copy', 'average', or 'linear' (feathering)\n",
    "        \"\"\"\n",
    "        self.blend_method = blend_method\n",
    "    \n",
    "    def warp_image(self, img: np.ndarray, H: np.ndarray, \n",
    "                   output_shape: Optional[Tuple[int, int]] = None) -> Tuple[np.ndarray, Tuple]:\n",
    "        \"\"\"\n",
    "        Warp image using homography matrix.\n",
    "        \n",
    "        Args:\n",
    "            img: Input image\n",
    "            H: 3x3 homography matrix\n",
    "            output_shape: Optional (height, width) for output canvas\n",
    "            \n",
    "        Returns:\n",
    "            warped: Warped image\n",
    "            offset: (x_offset, y_offset) to handle negative coordinates\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Find corners of source image\n",
    "        corners = np.array([\n",
    "            [0, 0],\n",
    "            [w-1, 0],\n",
    "            [w-1, h-1],\n",
    "            [0, h-1]\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # Transform corners to destination\n",
    "        corners_h = np.hstack([corners, np.ones((4, 1))])\n",
    "        corners_transformed = (H @ corners_h.T).T\n",
    "        corners_transformed = corners_transformed[:, :2] / corners_transformed[:, 2:3]\n",
    "        \n",
    "        # Find bounding box\n",
    "        min_x = np.floor(corners_transformed[:, 0].min()).astype(int)\n",
    "        max_x = np.ceil(corners_transformed[:, 0].max()).astype(int)\n",
    "        min_y = np.floor(corners_transformed[:, 1].min()).astype(int)\n",
    "        max_y = np.ceil(corners_transformed[:, 1].max()).astype(int)\n",
    "        \n",
    "        # Create translation matrix to handle negative coordinates\n",
    "        offset = (-min_x, -min_y)\n",
    "        T = np.array([\n",
    "            [1, 0, offset[0]],\n",
    "            [0, 1, offset[1]],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        # Combined transformation\n",
    "        H_translated = T @ H\n",
    "        \n",
    "        # Determine output size\n",
    "        if output_shape is None:\n",
    "            out_w = max_x - min_x\n",
    "            out_h = max_y - min_y\n",
    "        else:\n",
    "            out_h, out_w = output_shape\n",
    "        \n",
    "        # Warp image\n",
    "        warped = cv2.warpPerspective(img, H_translated, (out_w, out_h))\n",
    "        \n",
    "        return warped, offset\n",
    "    \n",
    "    def create_panorama_pair(self, img1: np.ndarray, img2: np.ndarray, \n",
    "                            H: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create panorama from two images.\n",
    "        \n",
    "        Args:\n",
    "            img1: Reference image\n",
    "            img2: Image to be warped onto img1's plane\n",
    "            H: Homography mapping img2 to img1\n",
    "            \n",
    "        Returns:\n",
    "            panorama: Stitched panorama image\n",
    "        \"\"\"\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        \n",
    "        # Find corners of both images in the panorama coordinate system\n",
    "        corners1 = np.array([\n",
    "            [0, 0],\n",
    "            [w1, 0],\n",
    "            [w1, h1],\n",
    "            [0, h1]\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        corners2 = np.array([\n",
    "            [0, 0],\n",
    "            [w2, 0],\n",
    "            [w2, h2],\n",
    "            [0, h2]\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # Transform corners2 using H\n",
    "        corners2_h = np.hstack([corners2, np.ones((4, 1))])\n",
    "        corners2_transformed = (H @ corners2_h.T).T\n",
    "        corners2_transformed = corners2_transformed[:, :2] / corners2_transformed[:, 2:3]\n",
    "        \n",
    "        # Combine all corners\n",
    "        all_corners = np.vstack([corners1, corners2_transformed])\n",
    "        \n",
    "        # Find bounding box\n",
    "        min_x = np.floor(all_corners[:, 0].min()).astype(int)\n",
    "        max_x = np.ceil(all_corners[:, 0].max()).astype(int)\n",
    "        min_y = np.floor(all_corners[:, 1].min()).astype(int)\n",
    "        max_y = np.ceil(all_corners[:, 1].max()).astype(int)\n",
    "        \n",
    "        # Translation to handle negative coordinates\n",
    "        offset = (-min_x, -min_y)\n",
    "        T = np.array([\n",
    "            [1, 0, offset[0]],\n",
    "            [0, 1, offset[1]],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        # Output size\n",
    "        out_w = max_x - min_x\n",
    "        out_h = max_y - min_y\n",
    "        \n",
    "        # Warp img2\n",
    "        warped2 = cv2.warpPerspective(img2, T @ H, (out_w, out_h))\n",
    "        \n",
    "        # Warp img1 (just translation)\n",
    "        warped1 = cv2.warpPerspective(img1, T, (out_w, out_h))\n",
    "        \n",
    "        # Blend images\n",
    "        panorama = self.blend_images(warped1, warped2)\n",
    "        \n",
    "        return panorama\n",
    "    \n",
    "    def blend_images(self, img1: np.ndarray, img2: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Blend two images based on selected method.\n",
    "        \n",
    "        Args:\n",
    "            img1: First warped image\n",
    "            img2: Second warped image\n",
    "            \n",
    "        Returns:\n",
    "            blended: Blended result\n",
    "        \"\"\"\n",
    "        # Create masks for non-zero regions\n",
    "        mask1 = (np.sum(img1, axis=2) > 0).astype(np.float32)\n",
    "        mask2 = (np.sum(img2, axis=2) > 0).astype(np.float32)\n",
    "        \n",
    "        # Find overlap region\n",
    "        overlap = np.logical_and(mask1, mask2).astype(np.float32)\n",
    "        \n",
    "        if self.blend_method == 'copy':\n",
    "            # Simple copy: img2 where available, else img1\n",
    "            result = img1.copy()\n",
    "            result[mask2 > 0] = img2[mask2 > 0]\n",
    "            \n",
    "        elif self.blend_method == 'average':\n",
    "            # Average in overlap region\n",
    "            result = img1.copy()\n",
    "            overlap_mask = overlap > 0\n",
    "            result[overlap_mask] = (img1[overlap_mask] + img2[overlap_mask]) // 2\n",
    "            result[np.logical_and(mask2 > 0, mask1 == 0)] = img2[np.logical_and(mask2 > 0, mask1 == 0)]\n",
    "            \n",
    "        elif self.blend_method == 'linear':\n",
    "            # Linear blending (feathering) in overlap region\n",
    "            # Create distance transforms\n",
    "            import scipy.ndimage as ndimage\n",
    "            \n",
    "            dist1 = ndimage.distance_transform_edt(mask1)\n",
    "            dist2 = ndimage.distance_transform_edt(mask2)\n",
    "            \n",
    "            # Normalize weights in overlap region\n",
    "            weights1 = dist1 / (dist1 + dist2 + 1e-10)\n",
    "            weights2 = dist2 / (dist1 + dist2 + 1e-10)\n",
    "            \n",
    "            # Expand dimensions for broadcasting\n",
    "            weights1 = weights1[:, :, np.newaxis]\n",
    "            weights2 = weights2[:, :, np.newaxis]\n",
    "            \n",
    "            # Blend\n",
    "            result = (weights2 * img1 + weights1 * img2).astype(np.uint8)\n",
    "            \n",
    "            # Fill non-overlap regions\n",
    "            result[np.logical_and(mask1 > 0, mask2 == 0)] = img1[np.logical_and(mask1 > 0, mask2 == 0)]\n",
    "            result[np.logical_and(mask2 > 0, mask1 == 0)] = img2[np.logical_and(mask2 > 0, mask1 == 0)]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown blend method: {self.blend_method}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_multi_image_panorama(self, images: List[np.ndarray], \n",
    "                                   homographies: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create panorama from multiple images.\n",
    "        \n",
    "        Args:\n",
    "            images: List of images (first is reference)\n",
    "            homographies: List of homographies mapping each image to reference\n",
    "                         (should have len(images)-1 entries)\n",
    "            \n",
    "        Returns:\n",
    "            panorama: Stitched panorama\n",
    "        \"\"\"\n",
    "        if len(images) < 2:\n",
    "            return images[0] if len(images) == 1 else None\n",
    "        \n",
    "        # Start with reference image\n",
    "        panorama = images[0]\n",
    "        \n",
    "        # Stitch each image sequentially\n",
    "        for i, (img, H) in enumerate(zip(images[1:], homographies)):\n",
    "            print(f\"Stitching image {i+2}/{len(images)}...\")\n",
    "            panorama = self.create_panorama_pair(panorama, img, H)\n",
    "        \n",
    "        return panorama\n",
    "\n",
    "print(\"PanoramaStitcher class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77b74b",
   "metadata": {},
   "source": [
    "## Complete Pipeline: Test on Sample Scene <a name=\"test\"></a>\n",
    "\n",
    "Let's test the complete pipeline on a sample scene before running on all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f29157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pipeline on v_bird scene with images 1 and 2\n",
    "test_scene = \"v_bird\"\n",
    "scene_path = PANORAMA_PATH / test_scene\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread(str(scene_path / \"1.png\"))\n",
    "img2 = cv2.imread(str(scene_path / \"2.png\"))\n",
    "\n",
    "print(f\"Testing pipeline on scene: {test_scene}\")\n",
    "print(f\"Image 1 shape: {img1.shape}\")\n",
    "print(f\"Image 2 shape: {img2.shape}\")\n",
    "\n",
    "# Test with SIFT\n",
    "print(\"\\n=== Testing with SIFT ===\")\n",
    "extractor = FeatureExtractor(method='SIFT')\n",
    "kp1, desc1 = extractor.detect_and_compute(img1)\n",
    "kp2, desc2 = extractor.detect_and_compute(img2)\n",
    "print(f\"Keypoints detected - Image 1: {len(kp1)}, Image 2: {len(kp2)}\")\n",
    "\n",
    "# Visualize keypoints\n",
    "_ = extractor.visualize_keypoints(img1, kp1, f\"{test_scene} - Image 1 Keypoints\")\n",
    "plt.savefig(OUTPUT_PATH / \"visualizations\" / f\"{test_scene}_img1_keypoints.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "_ = extractor.visualize_keypoints(img2, kp2, f\"{test_scene} - Image 2 Keypoints\")\n",
    "plt.savefig(OUTPUT_PATH / \"visualizations\" / f\"{test_scene}_img2_keypoints.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb653081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matching\n",
    "matcher = FeatureMatcher(descriptor_type='float', ratio_threshold=0.75)\n",
    "matches = matcher.match(desc1, desc2)\n",
    "print(f\"\\nMatches found: {len(matches)}\")\n",
    "\n",
    "# Visualize matches\n",
    "_ = matcher.visualize_matches(img1, kp1, img2, kp2, matches, \n",
    "                              f\"{test_scene} - Feature Matches\", max_display=100)\n",
    "plt.savefig(OUTPUT_PATH / \"visualizations\" / f\"{test_scene}_matches.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract matched point coordinates\n",
    "src_pts = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "# Homography estimation with RANSAC\n",
    "estimator = HomographyEstimator(ransac_iterations=2000, ransac_threshold=5.0)\n",
    "H, inlier_mask, inlier_ratio = estimator.estimate_homography_ransac(src_pts, dst_pts)\n",
    "\n",
    "print(f\"\\nHomography estimation:\")\n",
    "print(f\"Inliers: {np.sum(inlier_mask)}/{len(matches)} ({inlier_ratio*100:.2f}%)\")\n",
    "print(f\"Homography matrix:\\n{H}\")\n",
    "\n",
    "# Visualize inliers vs outliers\n",
    "_ = estimator.visualize_inliers(img1, kp1, img2, kp2, matches, inlier_mask,\n",
    "                                 f\"{test_scene} - Inliers vs Outliers\")\n",
    "plt.savefig(OUTPUT_PATH / \"visualizations\" / f\"{test_scene}_inliers.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create panorama\n",
    "stitcher = PanoramaStitcher(blend_method='linear')\n",
    "panorama = stitcher.create_panorama_pair(img1, img2, H)\n",
    "\n",
    "print(f\"\\nPanorama created with shape: {panorama.shape}\")\n",
    "\n",
    "# Visualize panorama\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(cv2.cvtColor(panorama, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"{test_scene} - Panorama\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / f\"{test_scene}_panorama_test.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Pipeline test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d92660",
   "metadata": {},
   "source": [
    "## Process All Scenes - Create Panoramas <a name=\"all-scenes\"></a>\n",
    "\n",
    "Now let's process all 6 scenes and create panoramas for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scene(scene_name: str, method: str = 'SIFT', num_images: int = 6):\n",
    "    \"\"\"\n",
    "    Process a complete scene and create panorama.\n",
    "    \n",
    "    Args:\n",
    "        scene_name: Name of the scene folder\n",
    "        method: Feature detection method\n",
    "        num_images: Number of images in the scene\n",
    "        \n",
    "    Returns:\n",
    "        results: Dictionary with panorama and statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing scene: {scene_name} with {method}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    scene_path = PANORAMA_PATH / scene_name\n",
    "    \n",
    "    # Load all images\n",
    "    images = []\n",
    "    for i in range(1, num_images + 1):\n",
    "        img_path = scene_path / f\"{i}.png\"\n",
    "        if img_path.exists():\n",
    "            img = cv2.imread(str(img_path))\n",
    "            images.append(img)\n",
    "        else:\n",
    "            print(f\"Warning: {img_path} not found\")\n",
    "    \n",
    "    if len(images) < 2:\n",
    "        print(f\"Insufficient images in {scene_name}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loaded {len(images)} images\")\n",
    "    \n",
    "    # Initialize components\n",
    "    extractor = FeatureExtractor(method=method)\n",
    "    matcher = FeatureMatcher(descriptor_type=extractor.descriptor_type, ratio_threshold=0.75)\n",
    "    estimator = HomographyEstimator(ransac_iterations=2000, ransac_threshold=5.0)\n",
    "    stitcher = PanoramaStitcher(blend_method='linear')\n",
    "    \n",
    "    # Extract features for all images\n",
    "    keypoints = []\n",
    "    descriptors = []\n",
    "    for i, img in enumerate(images):\n",
    "        kp, desc = extractor.detect_and_compute(img)\n",
    "        keypoints.append(kp)\n",
    "        descriptors.append(desc)\n",
    "        print(f\"Image {i+1}: {len(kp)} keypoints\")\n",
    "    \n",
    "    # Compute homographies relative to first image\n",
    "    homographies = []\n",
    "    statistics = []\n",
    "    \n",
    "    for i in range(1, len(images)):\n",
    "        print(f\"\\nEstimating homography: Image {i+1} -> Image 1\")\n",
    "        \n",
    "        # Match features\n",
    "        matches = matcher.match(descriptors[i], descriptors[0])\n",
    "        print(f\"  Matches: {len(matches)}\")\n",
    "        \n",
    "        if len(matches) < 4:\n",
    "            print(f\"  Warning: Insufficient matches for homography estimation\")\n",
    "            continue\n",
    "        \n",
    "        # Extract point coordinates\n",
    "        src_pts = np.float32([keypoints[i][m.queryIdx].pt for m in matches])\n",
    "        dst_pts = np.float32([keypoints[0][m.trainIdx].pt for m in matches])\n",
    "        \n",
    "        # Estimate homography\n",
    "        H, inlier_mask, inlier_ratio = estimator.estimate_homography_ransac(src_pts, dst_pts)\n",
    "        homographies.append(H)\n",
    "        \n",
    "        # Compute reprojection error\n",
    "        errors = estimator.compute_reprojection_error(src_pts[inlier_mask], \n",
    "                                                      dst_pts[inlier_mask], H)\n",
    "        avg_error = np.mean(errors)\n",
    "        \n",
    "        stats = {\n",
    "            'image_pair': f\"{i+1}->1\",\n",
    "            'matches': len(matches),\n",
    "            'inliers': np.sum(inlier_mask),\n",
    "            'inlier_ratio': inlier_ratio,\n",
    "            'avg_reprojection_error': avg_error\n",
    "        }\n",
    "        statistics.append(stats)\n",
    "        \n",
    "        print(f\"  Inliers: {stats['inliers']}/{stats['matches']} ({inlier_ratio*100:.2f}%)\")\n",
    "        print(f\"  Avg reprojection error: {avg_error:.3f} pixels\")\n",
    "    \n",
    "    # Create panorama\n",
    "    print(f\"\\nCreating panorama...\")\n",
    "    if len(homographies) > 0:\n",
    "        # Stitch images sequentially\n",
    "        panorama = images[0]\n",
    "        for i, H in enumerate(homographies):\n",
    "            print(f\"  Stitching image {i+2}...\")\n",
    "            panorama = stitcher.create_panorama_pair(panorama, images[i+1], H)\n",
    "        \n",
    "        # Save panorama\n",
    "        output_path = OUTPUT_PATH / f\"{scene_name}_panorama_{method}.png\"\n",
    "        cv2.imwrite(str(output_path), panorama)\n",
    "        print(f\"  Saved to: {output_path}\")\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(cv2.cvtColor(panorama, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"{scene_name} - Panorama ({method})\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_PATH / f\"{scene_name}_panorama_{method}_viz.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'scene': scene_name,\n",
    "            'method': method,\n",
    "            'panorama': panorama,\n",
    "            'statistics': statistics\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Scene processing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all scenes\n",
    "scenes = ['v_bird', 'v_boat', 'v_circus', 'v_graffiti', 'v_soldiers', 'v_weapons']\n",
    "all_results = []\n",
    "\n",
    "for scene in scenes:\n",
    "    result = process_scene(scene, method='SIFT', num_images=6)\n",
    "    if result:\n",
    "        all_results.append(result)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Completed processing {len(all_results)} scenes\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c9894",
   "metadata": {},
   "source": [
    "## Feature Detector Comparison <a name=\"comparison\"></a>\n",
    "\n",
    "Compare SIFT, SURF, and ORB on a sample scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_detectors(scene_name: str = 'v_bird'):\n",
    "    \"\"\"Compare SIFT, SURF, and ORB detectors on a scene.\"\"\"\n",
    "    \n",
    "    scene_path = PANORAMA_PATH / scene_name\n",
    "    img1 = cv2.imread(str(scene_path / \"1.png\"))\n",
    "    img2 = cv2.imread(str(scene_path / \"2.png\"))\n",
    "    \n",
    "    methods = ['SIFT', 'ORB']  # Skip SURF if not available\n",
    "    comparison_data = []\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {method}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Extract features\n",
    "            extractor = FeatureExtractor(method=method)\n",
    "            kp1, desc1 = extractor.detect_and_compute(img1)\n",
    "            kp2, desc2 = extractor.detect_and_compute(img2)\n",
    "            \n",
    "            # Match features\n",
    "            matcher = FeatureMatcher(descriptor_type=extractor.descriptor_type, ratio_threshold=0.75)\n",
    "            matches = matcher.match(desc1, desc2)\n",
    "            \n",
    "            # Estimate homography\n",
    "            if len(matches) >= 4:\n",
    "                src_pts = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "                dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "                \n",
    "                estimator = HomographyEstimator(ransac_iterations=1000, ransac_threshold=5.0)\n",
    "                H, inlier_mask, inlier_ratio = estimator.estimate_homography_ransac(src_pts, dst_pts)\n",
    "                \n",
    "                errors = estimator.compute_reprojection_error(src_pts[inlier_mask], \n",
    "                                                             dst_pts[inlier_mask], H)\n",
    "                avg_error = np.mean(errors)\n",
    "            else:\n",
    "                inlier_ratio = 0\n",
    "                avg_error = float('inf')\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            data = {\n",
    "                'Method': method,\n",
    "                'Keypoints (img1)': len(kp1),\n",
    "                'Keypoints (img2)': len(kp2),\n",
    "                'Matches': len(matches),\n",
    "                'Inliers': np.sum(inlier_mask) if len(matches) >= 4 else 0,\n",
    "                'Inlier Ratio (%)': f\"{inlier_ratio*100:.2f}\",\n",
    "                'Avg Reprojection Error': f\"{avg_error:.3f}\",\n",
    "                'Runtime (s)': f\"{elapsed_time:.3f}\"\n",
    "            }\n",
    "            comparison_data.append(data)\n",
    "            \n",
    "            print(f\"Keypoints: {len(kp1)}, {len(kp2)}\")\n",
    "            print(f\"Matches: {len(matches)}\")\n",
    "            print(f\"Inlier ratio: {inlier_ratio*100:.2f}%\")\n",
    "            print(f\"Runtime: {elapsed_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method}: {e}\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPARISON TABLE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run comparison\n",
    "comparison_df = compare_detectors('v_bird')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038634b1",
   "metadata": {},
   "source": [
    "## Augmented Reality Application <a name=\"ar\"></a>\n",
    "\n",
    "Implement the AR application that projects a source video onto a book cover in another video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180031d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARVideoProcessor:\n",
    "    \"\"\"\n",
    "    Augmented Reality video processor for projecting source video onto planar target.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_image_path: str, target_video_path: str, \n",
    "                 source_video_path: str, output_path: str):\n",
    "        \"\"\"\n",
    "        Initialize AR video processor.\n",
    "        \n",
    "        Args:\n",
    "            reference_image_path: Path to reference image (book cover)\n",
    "            target_video_path: Path to target video (book.mov)\n",
    "            source_video_path: Path to source video to project (ar_source.mov)\n",
    "            output_path: Path to save output video\n",
    "        \"\"\"\n",
    "        self.reference_image_path = reference_image_path\n",
    "        self.target_video_path = target_video_path\n",
    "        self.source_video_path = source_video_path\n",
    "        self.output_path = output_path\n",
    "        \n",
    "        # Load reference image\n",
    "        self.reference = cv2.imread(reference_image_path)\n",
    "        if self.reference is None:\n",
    "            raise ValueError(f\"Failed to load reference image: {reference_image_path}\")\n",
    "        \n",
    "        # Extract features from reference once\n",
    "        self.extractor = FeatureExtractor(method='SIFT')\n",
    "        self.ref_kp, self.ref_desc = self.extractor.detect_and_compute(self.reference)\n",
    "        print(f\"Reference image keypoints: {len(self.ref_kp)}\")\n",
    "        \n",
    "        # Initialize matcher and estimator\n",
    "        self.matcher = FeatureMatcher(descriptor_type='float', ratio_threshold=0.75)\n",
    "        self.estimator = HomographyEstimator(ransac_iterations=1000, ransac_threshold=5.0)\n",
    "        \n",
    "        # Open videos\n",
    "        self.target_cap = cv2.VideoCapture(target_video_path)\n",
    "        self.source_cap = cv2.VideoCapture(source_video_path)\n",
    "        \n",
    "        if not self.target_cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open target video: {target_video_path}\")\n",
    "        if not self.source_cap.isOpened():\n",
    "            raise ValueError(f\"Failed to open source video: {source_video_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        self.target_fps = self.target_cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.target_width = int(self.target_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.target_height = int(self.target_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.target_frame_count = int(self.target_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        self.source_frame_count = int(self.source_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"Target video: {self.target_width}x{self.target_height} @ {self.target_fps} fps, \"\n",
    "              f\"{self.target_frame_count} frames\")\n",
    "        print(f\"Source video: {self.source_frame_count} frames\")\n",
    "    \n",
    "    def get_book_corners(self, frame: np.ndarray, H: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get corners of the book in the frame using homography.\n",
    "        \n",
    "        Args:\n",
    "            frame: Current frame\n",
    "            H: Homography from reference to frame\n",
    "            \n",
    "        Returns:\n",
    "            corners: 4x2 array of corner coordinates\n",
    "        \"\"\"\n",
    "        h, w = self.reference.shape[:2]\n",
    "        ref_corners = np.array([\n",
    "            [0, 0],\n",
    "            [w, 0],\n",
    "            [w, h],\n",
    "            [0, h]\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # Transform corners\n",
    "        ref_corners_h = np.hstack([ref_corners, np.ones((4, 1))])\n",
    "        frame_corners_h = (H @ ref_corners_h.T).T\n",
    "        frame_corners = frame_corners_h[:, :2] / frame_corners_h[:, 2:3]\n",
    "        \n",
    "        return frame_corners.astype(np.int32)\n",
    "    \n",
    "    def crop_and_warp_source(self, source_frame: np.ndarray, H: np.ndarray, \n",
    "                            output_size: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Crop source frame to center and warp to match book perspective.\n",
    "        \n",
    "        Args:\n",
    "            source_frame: Source video frame\n",
    "            H: Homography from reference to target frame\n",
    "            output_size: (width, height) of reference\n",
    "            \n",
    "        Returns:\n",
    "            warped: Warped source frame\n",
    "        \"\"\"\n",
    "        src_h, src_w = source_frame.shape[:2]\n",
    "        ref_h, ref_w = output_size\n",
    "        \n",
    "        # Calculate aspect ratios\n",
    "        src_aspect = src_w / src_h\n",
    "        ref_aspect = ref_w / ref_h\n",
    "        \n",
    "        # Crop source to match reference aspect ratio (center crop)\n",
    "        if src_aspect > ref_aspect:\n",
    "            # Source is wider - crop width\n",
    "            new_w = int(src_h * ref_aspect)\n",
    "            x_offset = (src_w - new_w) // 2\n",
    "            cropped = source_frame[:, x_offset:x_offset + new_w]\n",
    "        else:\n",
    "            # Source is taller - crop height\n",
    "            new_h = int(src_w / ref_aspect)\n",
    "            y_offset = (src_h - new_h) // 2\n",
    "            cropped = source_frame[y_offset:y_offset + new_h, :]\n",
    "        \n",
    "        # Resize to reference size\n",
    "        resized = cv2.resize(cropped, (ref_w, ref_h))\n",
    "        \n",
    "        # Warp using homography\n",
    "        warped = cv2.warpPerspective(resized, H, (self.target_width, self.target_height))\n",
    "        \n",
    "        return warped\n",
    "    \n",
    "    def composite_frame(self, target_frame: np.ndarray, warped_source: np.ndarray, \n",
    "                       mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Composite warped source onto target frame.\n",
    "        \n",
    "        Args:\n",
    "            target_frame: Target video frame\n",
    "            warped_source: Warped source frame\n",
    "            mask: Binary mask of valid region\n",
    "            \n",
    "        Returns:\n",
    "            result: Composited frame\n",
    "        \"\"\"\n",
    "        result = target_frame.copy()\n",
    "        \n",
    "        # Apply warped source where mask is non-zero\n",
    "        mask_3ch = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR) if len(mask.shape) == 2 else mask\n",
    "        result = np.where(mask_3ch > 0, warped_source, result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_video(self, frame_skip: int = 1, max_frames: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Process videos and create AR output.\n",
    "        \n",
    "        Args:\n",
    "            frame_skip: Process every Nth frame (1 = all frames)\n",
    "            max_frames: Maximum number of frames to process (None = all)\n",
    "        \"\"\"\n",
    "        # Setup video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(self.output_path, fourcc, self.target_fps / frame_skip,\n",
    "                             (self.target_width, self.target_height))\n",
    "        \n",
    "        if not out.isOpened():\n",
    "            raise ValueError(f\"Failed to open video writer: {self.output_path}\")\n",
    "        \n",
    "        frame_idx = 0\n",
    "        processed_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        # Determine number of frames to process\n",
    "        total_frames = min(self.target_frame_count, self.source_frame_count)\n",
    "        if max_frames:\n",
    "            total_frames = min(total_frames, max_frames)\n",
    "        \n",
    "        print(f\"\\nProcessing {total_frames} frames (skip={frame_skip})...\")\n",
    "        \n",
    "        while True:\n",
    "            # Read target frame\n",
    "            ret_target, target_frame = self.target_cap.read()\n",
    "            if not ret_target:\n",
    "                break\n",
    "            \n",
    "            # Read source frame (loop if needed)\n",
    "            source_idx = frame_idx % self.source_frame_count\n",
    "            self.source_cap.set(cv2.CAP_PROP_POS_FRAMES, source_idx)\n",
    "            ret_source, source_frame = self.source_cap.read()\n",
    "            if not ret_source:\n",
    "                break\n",
    "            \n",
    "            # Skip frames if needed\n",
    "            if frame_idx % frame_skip != 0:\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "            \n",
    "            # Stop if max frames reached\n",
    "            if max_frames and processed_count >= max_frames:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Detect and match features\n",
    "                frame_kp, frame_desc = self.extractor.detect_and_compute(target_frame)\n",
    "                matches = self.matcher.match(self.ref_desc, frame_desc)\n",
    "                \n",
    "                if len(matches) < 4:\n",
    "                    print(f\"Frame {frame_idx}: Insufficient matches ({len(matches)})\")\n",
    "                    failed_count += 1\n",
    "                    out.write(target_frame)  # Write original frame\n",
    "                    frame_idx += 1\n",
    "                    processed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Estimate homography\n",
    "                src_pts = np.float32([self.ref_kp[m.queryIdx].pt for m in matches])\n",
    "                dst_pts = np.float32([frame_kp[m.trainIdx].pt for m in matches])\n",
    "                \n",
    "                H, inlier_mask, inlier_ratio = self.estimator.estimate_homography_ransac(src_pts, dst_pts)\n",
    "                \n",
    "                if inlier_ratio < 0.1:  # Too few inliers\n",
    "                    print(f\"Frame {frame_idx}: Low inlier ratio ({inlier_ratio*100:.1f}%)\")\n",
    "                    failed_count += 1\n",
    "                    out.write(target_frame)\n",
    "                    frame_idx += 1\n",
    "                    processed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Warp source frame\n",
    "                ref_h, ref_w = self.reference.shape[:2]\n",
    "                warped_source = self.crop_and_warp_source(source_frame, H, (ref_w, ref_h))\n",
    "                \n",
    "                # Create mask for valid region\n",
    "                mask = (np.sum(warped_source, axis=2) > 0).astype(np.uint8) * 255\n",
    "                \n",
    "                # Composite\n",
    "                result = self.composite_frame(target_frame, warped_source, mask)\n",
    "                \n",
    "                # Write frame\n",
    "                out.write(result)\n",
    "                \n",
    "                if processed_count % 10 == 0:\n",
    "                    print(f\"Processed frame {frame_idx}/{total_frames} \"\n",
    "                          f\"({processed_count} written, {failed_count} failed, \"\n",
    "                          f\"inliers: {inlier_ratio*100:.1f}%)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Frame {frame_idx} error: {e}\")\n",
    "                failed_count += 1\n",
    "                out.write(target_frame)\n",
    "            \n",
    "            frame_idx += 1\n",
    "            processed_count += 1\n",
    "        \n",
    "        # Cleanup\n",
    "        self.target_cap.release()\n",
    "        self.source_cap.release()\n",
    "        out.release()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"AR video processing complete!\")\n",
    "        print(f\"Total frames processed: {processed_count}\")\n",
    "        print(f\"Failed frames: {failed_count}\")\n",
    "        print(f\"Output saved to: {self.output_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "print(\"ARVideoProcessor class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4278c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process AR video\n",
    "# NOTE: This is computationally intensive. Start with a small number of frames for testing.\n",
    "\n",
    "reference_img = str(AR_PATH / \"cv_cover.jpg\")\n",
    "target_video = str(AR_PATH / \"book.mov\")\n",
    "source_video = str(AR_PATH / \"ar_source.mov\")\n",
    "output_video = str(OUTPUT_PATH / \"ar_dynamic_result.mp4\")\n",
    "\n",
    "print(\"Initializing AR video processor...\")\n",
    "print(\"NOTE: Processing full video may take several hours.\")\n",
    "print(\"Starting with first 30 frames for testing...\")\n",
    "\n",
    "ar_processor = ARVideoProcessor(reference_img, target_video, source_video, output_video)\n",
    "\n",
    "# Process video (start with limited frames for testing)\n",
    "# Change max_frames=None to process entire video\n",
    "ar_processor.process_video(frame_skip=1, max_frames=30)\n",
    "\n",
    "print(\"\\nAR video processing test complete!\")\n",
    "print(\"To process the full video, set max_frames=None in the process_video() call above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de06e0",
   "metadata": {},
   "source": [
    "## Summary and Statistics <a name=\"summary\"></a>\n",
    "\n",
    "Generate summary statistics and comparison tables for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics table\n",
    "print(\"=\"*80)\n",
    "print(\"PANORAMA STITCHING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in all_results:\n",
    "    print(f\"\\n{result['scene']} ({result['method']}):\")\n",
    "    print(\"-\" * 60)\n",
    "    for stats in result['statistics']:\n",
    "        print(f\"  {stats['image_pair']:>6}: \"\n",
    "              f\"Matches: {stats['matches']:>4}, \"\n",
    "              f\"Inliers: {stats['inliers']:>4} ({stats['inlier_ratio']*100:>5.1f}%), \"\n",
    "              f\"Error: {stats['avg_reprojection_error']:>6.3f}px\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ASSIGNMENT COMPLETION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Part 1: Feature Extraction (SIFT, SURF, ORB) - Implemented\")\n",
    "print(\"✓ Part 2: Feature Matching with Lowe's ratio test - Implemented\")\n",
    "print(\"✓ Part 3: DLT + RANSAC Homography Estimation - Implemented\")\n",
    "print(\"✓ Part 4: Image Warping and Panorama Stitching - Implemented\")\n",
    "print(\"✓ AR Application: Video projection on moving target - Implemented\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999dcee",
   "metadata": {},
   "source": [
    "## Instructions and Next Steps\n",
    "\n",
    "### What has been implemented:\n",
    "\n",
    "1. **Feature Extraction (Part 1)**: Complete implementation of SIFT, SURF, and ORB detectors with visualization\n",
    "2. **Feature Matching (Part 2)**: k-NN matching with Lowe's ratio test and optional cross-checking\n",
    "3. **Homography Estimation (Part 3)**: Manual DLT algorithm with point normalization and RANSAC\n",
    "4. **Panorama Stitching (Part 4)**: Image warping with multiple blending methods (copy, average, linear feathering)\n",
    "5. **AR Application**: Video projection framework with per-frame homography estimation\n",
    "\n",
    "### To complete the assignment:\n",
    "\n",
    "1. **Run all cells sequentially** - Start from the top and execute each cell\n",
    "2. **Install required packages** if needed: `opencv-contrib-python` for SURF (optional), `scipy` for distance transforms\n",
    "3. **Process all scenes** - The notebook will create panoramas for all 6 scenes\n",
    "4. **AR Video Processing** - Currently set to process 30 frames for testing. To process full video:\n",
    "   - Change `max_frames=30` to `max_frames=None` in the AR processing cell\n",
    "   - **Warning**: This may take several hours depending on your hardware\n",
    "   - Consider using `frame_skip=2` or `frame_skip=3` to speed up processing\n",
    "5. **Upload videos** - Upload `ar_dynamic_result.mp4` to Google Drive/OneDrive and include link in report\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- `panorama_results/[scene]_panorama_SIFT.png` - Panorama for each scene\n",
    "- `panorama_results/visualizations/` - Keypoint and matching visualizations\n",
    "- `panorama_results/ar_dynamic_result.mp4` - AR video output\n",
    "\n",
    "### Report Structure (as per assignment):\n",
    "\n",
    "1. **Overview**: Summarize the pipeline and objectives\n",
    "2. **Dataset & Setup**: Describe panorama_dataset and ar_dataset\n",
    "3. **Methods**: \n",
    "   - Explain SIFT/SURF/ORB choices and properties\n",
    "   - Describe matching strategy (k-NN, ratio test)\n",
    "   - Detail DLT derivation and RANSAC parameters\n",
    "   - Explain warping and blending techniques\n",
    "   - Describe AR per-frame processing\n",
    "4. **Results**: Include all visualizations generated by this notebook\n",
    "5. **Discussion**: Analyze when the pipeline works well vs. fails\n",
    "6. **Reproducibility**: List all parameters used\n",
    "\n",
    "### Key Parameters Used:\n",
    "\n",
    "- **Lowe's ratio threshold**: 0.75\n",
    "- **RANSAC iterations**: 2000 (panorama), 1000 (AR)\n",
    "- **RANSAC inlier threshold**: 5.0 pixels\n",
    "- **Point normalization**: Enabled (improves numerical stability)\n",
    "- **Blending method**: Linear (feathering) for smooth transitions\n",
    "- **ORB keypoints**: 5000 maximum\n",
    "- **Random seed**: 42 (for reproducibility)\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- Check `OUTPUT_PATH` for all generated files\n",
    "- Use comparison table to analyze SIFT vs ORB performance\n",
    "- Save intermediate results frequently\n",
    "- For AR video, verify first few frames before processing full video\n",
    "- Consider optical flow tracking (cv2.calcOpticalFlowPyrLK) for more stable AR if needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
